{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "177.188px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "GAN_Fashion_MNIST_Draft01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fppU7vrfGoJf",
        "colab_type": "text"
      },
      "source": [
        "# Week 06 | Siraj Raval | MMWML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T06:57:05.206457Z",
          "start_time": "2019-10-05T06:57:04.666395Z"
        },
        "id": "_vU9PzF4GoJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T06:57:06.422086Z",
          "start_time": "2019-10-05T06:57:05.599477Z"
        },
        "id": "MOXw-Fp7GoJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5df2589b-fb87-4b62-cbc6-7cdb667d630b"
      },
      "source": [
        "!pip install tensorflow-gpu==1.14\n",
        "clear_output()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwE5cxqaGoJq",
        "colab_type": "text"
      },
      "source": [
        "## Homework Statement\n",
        "\n",
        "The homework for this week is to create a Generative Adversarial Network that will be able to generate novel images after training. The dataset to be used for this is [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMLpTy2NGoJr",
        "colab_type": "text"
      },
      "source": [
        "## 0 References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M90BxQ7SGoJs",
        "colab_type": "text"
      },
      "source": [
        "### 0.1 Project Help\n",
        "\n",
        "- Rowel Atienza's [book](https://www.amazon.com/Advanced-Deep-Learning-Keras-reinforcement/dp/1788629418/) and [Medium article](https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0) provided my fundamental understanding of implementation of a simple Deep Convolutional GAN (DC-GAN)\n",
        "- Rowel also attached a sample python [script](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/Experiments/Tensorflow/GAN/dcgan_mnist.py) in the Medium article referenced above which had a template of the code\n",
        "\n",
        "- Github User `R-Suresh`'s [code](https://github.com/R-Suresh/GAN_fashion_MNIST/blob/master/gan.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uRTD-2bGoJv",
        "colab_type": "text"
      },
      "source": [
        "### 0.2 Alternative to MNIST dataset: Fashion-MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhtFv0w3GoJw",
        "colab_type": "text"
      },
      "source": [
        "___Introducing the Fashion-MNIST___\n",
        "\n",
        "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) is a dataset of [Zalando's](https://jobs.zalando.com/en/) article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the [original MNIST dataset](http://yann.lecun.com/exdb/mnist/) for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
        "\n",
        "<img src = \"https://tensorflow.org/images/fashion-mnist-sprite.png\" width = 450 height = 400>\n",
        "\n",
        "<br>\n",
        "\n",
        "___Why was the Fashion-MNIST created?___\n",
        "\n",
        "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. _\"If it doesn't work on MNIST, it won't work at all\"_ , they said. \n",
        "\n",
        "_\"Well, if it does work on MNIST, it may still fail on others.\"_\n",
        "\n",
        "<img src = \"https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/embedding.gif\">\n",
        "\n",
        "Here are some good reasons to replace MNIST:\n",
        "\n",
        "- __MNIST is too easy__: Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily.\n",
        "- __MNIST is overused__: In [this April 2017 Twitter thread](https://twitter.com/goodfellow_ian/status/852591106655043584), Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\n",
        "- __MNIST can not represent modern CV tasks__, as noted in [this April 2017 Twitter thread](https://twitter.com/fchollet/status/852594987527045120), deep learning expert/Keras author François Chollet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T06:59:18.828517Z",
          "start_time": "2019-10-05T06:59:08.752184Z"
        },
        "id": "-BgiPdtjGoJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cd240d14-6f22-4cb3-9245-2354b22d1d0a"
      },
      "source": [
        "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T07:00:10.719419Z",
          "start_time": "2019-10-05T07:00:10.711161Z"
        },
        "id": "iO9eWYrGGoJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6a6e208-8cee-456c-bb1a-9a4815b6434b"
      },
      "source": [
        "# shape of the image datasets\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (10000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T07:00:28.254755Z",
          "start_time": "2019-10-05T07:00:28.250244Z"
        },
        "id": "i7-oTktUGoJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e2b3d21-24f0-4d20-d23c-6bf1ec111735"
      },
      "source": [
        "# shape of the label vectors\n",
        "y_train.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000,), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-05T07:01:59.217184Z",
          "start_time": "2019-10-05T07:01:59.210033Z"
        },
        "id": "_qwiCDL9GoJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbf75176-9c0c-48b2-cba5-545085a601cf"
      },
      "source": [
        "# values in the label vectors\n",
        "\n",
        "class_ids = np.unique(y_train)\n",
        "class_ids"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1I_4xLGoJ6",
        "colab_type": "text"
      },
      "source": [
        "___Actual labels for the numbers in `class_ids`___\n",
        "\n",
        "| 0 | T-shirt/top |\n",
        "|:-:|:-----------:|\n",
        "| 1 |   Trouser   |\n",
        "| 2 |   Pullover  |\n",
        "| 3 |    Dress    |\n",
        "| 4 |     Coat    |\n",
        "| 5 |    Sandal   |\n",
        "| 6 |    Shirt    |\n",
        "| 7 |   Sneaker   |\n",
        "| 8 |     Bag     |\n",
        "| 9 |  Ankle Boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh1bMB1mGoJ6",
        "colab_type": "text"
      },
      "source": [
        "## 1 Creating a GAN model using `tf.keras`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4s731WDGoJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "import sys\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss = 'binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        #model.add(Dense(2048))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        #model.add(Dense(512))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "    \n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        #fig.savefig(\"images/%d.png\" % epoch)\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KuLzS2iIV5J",
        "colab_type": "text"
      },
      "source": [
        "## 2 Training the GAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SftRmGLGHSzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(GAN_object, epochs, batch_size = 256, sample_interval = 50):\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = x_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, GAN_object.latent_dim))\n",
        "\n",
        "            # Generate a batch of new images\n",
        "            gen_imgs = GAN_object.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = GAN_object.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = GAN_object.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, GAN_object.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = GAN_object.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                GAN_object.sample_images(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6vj_0C8HS1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAN_object = GAN()\n",
        "train(GAN_object, epochs = 3000, batch_size = 256, sample_interval = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asCkWki6O8OD",
        "colab_type": "text"
      },
      "source": [
        "`2990 [D loss: 0.641810, acc.: 62.30%] [G loss: 0.823704]`\n",
        "\n",
        "`2991 [D loss: 0.659239, acc.: 59.38%] [G loss: 0.845749]`\n",
        "\n",
        "`2992 [D loss: 0.666248, acc.: 58.01%] [G loss: 0.801034]`\n",
        "\n",
        "`2993 [D loss: 0.659432, acc.: 58.20%] [G loss: 0.825578]`\n",
        "\n",
        "`2994 [D loss: 0.657774, acc.: 58.20%] [G loss: 0.826891]`\n",
        "\n",
        "`2995 [D loss: 0.669790, acc.: 55.66%] [G loss: 0.825897]`\n",
        "\n",
        "`2996 [D loss: 0.675206, acc.: 52.73%] [G loss: 0.828771]`\n",
        "\n",
        "`2997 [D loss: 0.659878, acc.: 59.57%] [G loss: 0.840454]`\n",
        "\n",
        "`2998 [D loss: 0.664810, acc.: 59.57%] [G loss: 0.848437]`\n",
        "\n",
        "`2999 [D loss: 0.667068, acc.: 60.16%] [G loss: 0.835160]`"
      ]
    }
  ]
}